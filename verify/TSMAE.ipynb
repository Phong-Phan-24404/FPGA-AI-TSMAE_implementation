{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-10T18:19:15.352985Z",
     "iopub.status.busy": "2025-05-10T18:19:15.352685Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       att1      att2      att3      att4      att5      att6      att7  \\\n",
      "0 -0.112522 -2.827204 -3.773897 -4.349751 -4.376041 -3.474986 -2.181408   \n",
      "1 -1.100878 -3.996840 -4.285843 -4.506579 -4.022377 -3.234368 -1.566126   \n",
      "2 -0.567088 -2.593450 -3.874230 -4.584095 -4.187449 -3.151462 -1.742940   \n",
      "3  0.490473 -1.914407 -3.616364 -4.318823 -4.268016 -3.881110 -2.993280   \n",
      "4  0.800232 -0.874252 -2.384761 -3.973292 -4.338224 -3.802422 -2.534510   \n",
      "\n",
      "       att8      att9     att10  ...    att132    att133    att134    att135  \\\n",
      "0 -1.818286 -1.250522 -0.477492  ...  0.792168  0.933541  0.796958  0.578621   \n",
      "1 -0.992258 -0.754680  0.042321  ...  0.538356  0.656881  0.787490  0.724046   \n",
      "2 -1.490659 -1.183580 -0.394229  ...  0.886073  0.531452  0.311377 -0.021919   \n",
      "3 -1.671131 -1.333884 -0.965629  ...  0.350816  0.499111  0.600345  0.842069   \n",
      "4 -1.783423 -1.594450 -0.753199  ...  1.148884  0.958434  1.059025  1.371682   \n",
      "\n",
      "     att136    att137    att138    att139    att140  target  \n",
      "0  0.257740  0.228077  0.123431  0.925286  0.193137    b'1'  \n",
      "1  0.555784  0.476333  0.773820  1.119621 -1.436250    b'1'  \n",
      "2 -0.713683 -0.532197  0.321097  0.904227 -0.421797    b'1'  \n",
      "3  0.952074  0.990133  1.086798  1.403011 -0.383564    b'1'  \n",
      "4  1.277392  0.960304  0.971020  1.614392  1.421456    b'1'  \n",
      "\n",
      "[5 rows x 141 columns]\n",
      "Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 500/500 [00:03<00:00, 146.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.745244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 500/500 [00:03<00:00, 163.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.620413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 500/500 [00:03<00:00, 150.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.600558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 500/500 [00:03<00:00, 133.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.597288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 500/500 [00:03<00:00, 131.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.593601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 500/500 [00:03<00:00, 159.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.595682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 500/500 [00:03<00:00, 153.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.594660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 500/500 [00:03<00:00, 152.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.592216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 500/500 [00:03<00:00, 157.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.588220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 500/500 [00:03<00:00, 153.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.589200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 500/500 [00:03<00:00, 143.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Loss: 0.587827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 500/500 [00:03<00:00, 136.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss: 0.585673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 500/500 [00:03<00:00, 148.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Loss: 0.535913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 500/500 [00:03<00:00, 147.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss: 0.544370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 500/500 [00:03<00:00, 159.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Loss: 0.506451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 500/500 [00:03<00:00, 165.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss: 0.499451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 500/500 [00:03<00:00, 166.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 Loss: 0.498721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 500/500 [00:02<00:00, 168.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss: 0.501007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 500/500 [00:03<00:00, 165.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 Loss: 0.582376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 500/500 [00:03<00:00, 165.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss: 0.623200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 500/500 [00:02<00:00, 167.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 Loss: 0.614328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 500/500 [00:02<00:00, 168.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Loss: 0.599363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 500/500 [00:03<00:00, 164.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 Loss: 0.597636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 500/500 [00:03<00:00, 162.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 Loss: 0.593195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 500/500 [00:03<00:00, 145.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 Loss: 0.591028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 500/500 [00:03<00:00, 157.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 Loss: 0.585882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 500/500 [00:03<00:00, 165.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Loss: 0.585745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 500/500 [00:03<00:00, 165.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 Loss: 0.584776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 500/500 [00:03<00:00, 164.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 Loss: 0.583731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 500/500 [00:03<00:00, 162.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 Loss: 0.584033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 500/500 [00:03<00:00, 166.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Loss: 0.584210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 500/500 [00:03<00:00, 165.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 Loss: 0.583340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 500/500 [00:03<00:00, 162.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Loss: 0.583769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 500/500 [00:03<00:00, 163.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 Loss: 0.583466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 500/500 [00:03<00:00, 153.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 Loss: 0.583746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 500/500 [00:03<00:00, 157.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Loss: 0.583160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 500/500 [00:03<00:00, 163.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 Loss: 0.583059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 500/500 [00:03<00:00, 165.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 Loss: 0.583251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 500/500 [00:03<00:00, 161.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 Loss: 0.582610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 500/500 [00:03<00:00, 162.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Loss: 0.582543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 500/500 [00:03<00:00, 163.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 Loss: 0.583788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 500/500 [00:03<00:00, 164.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 Loss: 0.582602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 500/500 [00:03<00:00, 162.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 Loss: 0.582701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 500/500 [00:03<00:00, 162.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 Loss: 0.582856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|██████████| 500/500 [00:03<00:00, 165.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Loss: 0.582414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 500/500 [00:03<00:00, 157.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 Loss: 0.582014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|██████████| 500/500 [00:02<00:00, 167.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 Loss: 0.583042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|██████████| 500/500 [00:02<00:00, 168.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 Loss: 0.582165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|██████████| 500/500 [00:03<00:00, 158.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 Loss: 0.581662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|██████████| 500/500 [00:03<00:00, 162.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 Loss: 0.582174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|██████████| 500/500 [00:02<00:00, 166.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 Loss: 0.582214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|██████████| 500/500 [00:03<00:00, 165.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 Loss: 0.581935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|██████████| 500/500 [00:03<00:00, 164.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 Loss: 0.582122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|██████████| 500/500 [00:02<00:00, 167.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 Loss: 0.582516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|██████████| 500/500 [00:03<00:00, 163.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 Loss: 0.581897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|██████████| 500/500 [00:03<00:00, 149.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 Loss: 0.582287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|██████████| 500/500 [00:03<00:00, 162.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 Loss: 0.582475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100: 100%|██████████| 500/500 [00:02<00:00, 166.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58 Loss: 0.582179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100: 100%|██████████| 500/500 [00:03<00:00, 166.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59 Loss: 0.582294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100: 100%|██████████| 500/500 [00:02<00:00, 171.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60 Loss: 0.582476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100: 100%|██████████| 500/500 [00:03<00:00, 162.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61 Loss: 0.582242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100: 100%|██████████| 500/500 [00:03<00:00, 165.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62 Loss: 0.582867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100: 100%|██████████| 500/500 [00:02<00:00, 166.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63 Loss: 0.582159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100: 100%|██████████| 500/500 [00:02<00:00, 169.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64 Loss: 0.581965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100: 100%|██████████| 500/500 [00:03<00:00, 164.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65 Loss: 0.582371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100: 100%|██████████| 500/500 [00:03<00:00, 164.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66 Loss: 0.582469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100: 100%|██████████| 500/500 [00:03<00:00, 153.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67 Loss: 0.582025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100: 100%|██████████| 500/500 [00:03<00:00, 166.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68 Loss: 0.568710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100: 100%|██████████| 500/500 [00:03<00:00, 163.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69 Loss: 0.545989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/100: 100%|██████████| 500/500 [00:02<00:00, 168.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70 Loss: 0.518016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100: 100%|██████████| 500/500 [00:02<00:00, 169.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71 Loss: 0.508571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100: 100%|██████████| 500/500 [00:02<00:00, 170.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72 Loss: 0.509704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100: 100%|██████████| 500/500 [00:03<00:00, 162.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73 Loss: 0.506042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100: 100%|██████████| 500/500 [00:02<00:00, 168.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74 Loss: 0.502779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100: 100%|██████████| 500/500 [00:03<00:00, 164.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75 Loss: 0.500743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100: 100%|██████████| 500/500 [00:03<00:00, 164.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76 Loss: 0.500287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100: 100%|██████████| 500/500 [00:03<00:00, 160.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77 Loss: 0.505400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100: 100%|██████████| 500/500 [00:03<00:00, 154.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 Loss: 0.500494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100: 100%|██████████| 500/500 [00:03<00:00, 165.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79 Loss: 0.498066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100: 100%|██████████| 500/500 [00:02<00:00, 168.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80 Loss: 0.497615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100: 100%|██████████| 500/500 [00:03<00:00, 166.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81 Loss: 0.495677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100: 100%|██████████| 500/500 [00:02<00:00, 166.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82 Loss: 0.495895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100: 100%|██████████| 500/500 [00:02<00:00, 166.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83 Loss: 0.495126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100: 100%|██████████| 500/500 [00:02<00:00, 167.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84 Loss: 0.496322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100:  31%|███       | 153/500 [00:00<00:02, 163.42it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io.arff as arff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset Definition (ECG5000)\n",
    "# ---------------------------\n",
    "class ECG5000(Dataset):\n",
    "    def __init__(self, mode, split='train'):\n",
    "        \"\"\"\n",
    "        mode: 'normal', 'anomaly', or 'all'. \n",
    "              'all' means do not filter any samples (both normal and anomaly).\n",
    "        split: 'train' to load training data; 'test' to load test data.\n",
    "        \"\"\"\n",
    "        assert mode in ['normal', 'anomaly', 'all']\n",
    "        assert split in ['train', 'test']\n",
    "        \n",
    "        # Select the file based on the split.\n",
    "        if split == 'train':\n",
    "            file_path = '/kaggle/input/ecg50000/ECG5000_TRAIN.arff'\n",
    "        else:\n",
    "            file_path = '/kaggle/input/ecg50000/ECG5000_TEST.arff'\n",
    "        \n",
    "        data, meta = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data, columns=meta.names())\n",
    "        print(df.head())\n",
    "        # Rename the label column.\n",
    "        new_columns = list(df.columns)\n",
    "        new_columns[-1] = 'target'\n",
    "        df.columns = new_columns\n",
    "        \n",
    "        # Filter samples based on mode.\n",
    "        if mode == 'normal':\n",
    "            df = df[df.target == b'1'].drop(labels='target', axis=1)\n",
    "        elif mode == 'anomaly':\n",
    "            df = df[df.target != b'1'].drop(labels='target', axis=1)\n",
    "        else:  # mode == 'all'\n",
    "            df = df.drop(labels='target', axis=1)\n",
    "        \n",
    "        # Convert DataFrame to a numpy array of type float32.\n",
    "        self.X = df.astype(np.float32).to_numpy()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Each sample is reshaped as (sequence_length, 1)\n",
    "        sample = torch.from_numpy(self.X[index]).unsqueeze(-1)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def get_torch_tensor(self):\n",
    "        return torch.from_numpy(self.X)\n",
    "\n",
    "class MemoryModule(nn.Module):\n",
    "    def __init__(self, memory_size, hidden_size, sparsity_threshold=0.05):\n",
    "        \"\"\"\n",
    "        memory_size: Number of memory items.\n",
    "        hidden_size: Dimensionality of each memory item.\n",
    "        sparsity_threshold: Threshold for rectifying the addressing vector.\n",
    "        \"\"\"\n",
    "        super(MemoryModule, self).__init__()\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sparsity_threshold = sparsity_threshold\n",
    "        # Initialize learnable memory items.\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, hidden_size))\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z: latent representation from encoder with shape (batch, hidden_size)\n",
    "        Returns:\n",
    "          z_hat: recombined latent representation from memory.\n",
    "          q: sparse addressing vector with shape (batch, memory_size)\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute similarity scores between latent vector and memory items.\n",
    "        sim = torch.matmul(z, self.memory.t())  # shape: (batch, memory_size)\n",
    "        # Softmax to obtain addressing weights.\n",
    "        q = nn.functional.softmax(sim, dim=1)\n",
    "        # Rectify: subtract threshold and zero out negatives.\n",
    "        q = torch.max(q - self.sparsity_threshold, torch.zeros_like(q))\n",
    "        # Normalize so that each row sums to 1.\n",
    "        q = q / (q.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        # Recombine memory items.\n",
    "        z_hat = torch.matmul(q, self.memory)\n",
    "        return z_hat, q\n",
    "\n",
    "# ---------------------------\n",
    "# TSMAE Model Definition\n",
    "# ---------------------------\n",
    "class TSMAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, memory_size, sparsity_threshold=0.05, sparsity_factor=0.001):\n",
    "        \"\"\"\n",
    "        input_size: Dimension of each time step (e.g., 1)\n",
    "        hidden_size: Dimension of the latent representation\n",
    "        memory_size: Number of memory items.\n",
    "        sparsity_threshold: Threshold used in the memory module.\n",
    "        sparsity_factor: Weight for the sparsity penalty in the loss.\n",
    "        \"\"\"\n",
    "        super(TSMAE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.memory_size = memory_size\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        \n",
    "        # LSTM Encoder: encodes input sequence into a latent vector.\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        # Memory Module: extracts typical normal patterns.\n",
    "        self.memory_module = MemoryModule(memory_size, hidden_size, sparsity_threshold)\n",
    "        # LSTM Decoder: decodes the latent representation back to sequence.\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Final layer to project the LSTM decoder output to the input space.\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input tensor of shape (batch, seq_len, input_size)\n",
    "        Returns:\n",
    "          x_recon: Reconstructed sequence of shape (batch, seq_len, input_size)\n",
    "          q: Sparse addressing vector from the memory module (batch, memory_size)\n",
    "          z: Latent representation from the encoder (batch, hidden_size)\n",
    "          z_hat: Recombined latent representation from the memory module (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # Encode input sequence.\n",
    "        enc_out, (h_n, c_n) = self.encoder(x)\n",
    "        z = h_n[-1]  # Use the final hidden state; shape: (batch, hidden_size)\n",
    "        \n",
    "        # Pass through memory module.\n",
    "        z_hat, q = self.memory_module(z)\n",
    "        \n",
    "        # For decoding, repeat z_hat across the sequence length.\n",
    "        z_hat_seq = z_hat.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        dec_out, _ = self.decoder(z_hat_seq)\n",
    "        # Project decoder output back to input dimension.\n",
    "        x_recon = self.output_layer(dec_out)\n",
    "        return x_recon, q, z, z_hat\n",
    "\n",
    "    def loss_function(self, x, x_recon, q):\n",
    "        # Mean Squared Error reconstruction loss.\n",
    "        rec_loss = torch.mean((x - x_recon)**2)\n",
    "        # Sparsity loss to encourage a sparse addressing vector.\n",
    "        sparsity_loss = torch.mean(torch.log(1 + q**2))\n",
    "        loss = rec_loss + self.sparsity_factor * sparsity_loss\n",
    "        return loss, rec_loss, sparsity_loss\n",
    "\n",
    "# ---------------------------\n",
    "# Training Setup\n",
    "# ---------------------------\n",
    "def train_model(model, dataloader, optimizer, device, num_epochs=50):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, q, z, z_hat = model(batch)\n",
    "            loss, rec_loss, sparsity_loss = model.loss_function(batch, x_recon, q)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch.size(0)\n",
    "        avg_loss = epoch_loss / len(dataloader.dataset)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} Loss: {avg_loss:.6f}\")\n",
    "    return train_losses\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters.\n",
    "    input_size = 1           # Each time step has 1 feature.\n",
    "    hidden_size = 10         # Latent representation dimension.\n",
    "    memory_size = 1         # Number of memory items.\n",
    "    sparsity_threshold = 0.05\n",
    "    sparsity_factor = 0.001\n",
    "    batch_size = 1\n",
    "    num_epochs = 100\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training: Use the TRAIN file with both normal and anomaly samples.\n",
    "    # ---------------------------\n",
    "    train_dataset = ECG5000(mode='all', split='train')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize the TSMAE model.\n",
    "    model = TSMAE(input_size, hidden_size, memory_size, sparsity_threshold, sparsity_factor)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Check for available device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Training on device:\", device)\n",
    "\n",
    "    # Train the model.\n",
    "    train_losses = train_model(model, train_loader, optimizer, device, num_epochs=num_epochs)\n",
    "\n",
    "    # Plot training loss.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training Loss vs Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Save the Model Weights\n",
    "    # ---------------------------\n",
    "    torch.save(model.state_dict(), 'tsmae_weights.pth')\n",
    "    print(\"Model weights saved to 'tsmae_weights.pth'\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Evaluation: Use the TEST file with both normal and anomaly samples.\n",
    "    # ---------------------------\n",
    "    test_dataset = ECG5000(mode='all', split='test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    total_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            x_recon, q, z, z_hat = model(batch)\n",
    "            loss, rec_loss, sparsity_loss = model.loss_function(batch, x_recon, q)\n",
    "            total_test_loss += loss.item() * batch.size(0)\n",
    "    avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "    print(f\"Average test loss on TEST file: {avg_test_loss:.6f}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plotting Reconstruction Comparisons (TEST file)\n",
    "    # ---------------------------\n",
    "    # Evaluate one normal sample from the test file.\n",
    "    normal_dataset_eval = ECG5000(mode='normal', split='test')\n",
    "    normal_sample = normal_dataset_eval[0].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        normal_recon, normal_q, _, _ = model(normal_sample)\n",
    "    normal_series_np = normal_sample.cpu().numpy().flatten()\n",
    "    normal_recon_np = normal_recon.cpu().numpy().flatten()\n",
    "\n",
    "    # Evaluate one anomaly sample from the test file.\n",
    "    anomaly_dataset_eval = ECG5000(mode='anomaly', split='test')\n",
    "    anomaly_sample = anomaly_dataset_eval[0].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        anomaly_recon, anomaly_q, _, _ = model(anomaly_sample)\n",
    "    anomaly_series_np = anomaly_sample.cpu().numpy().flatten()\n",
    "    anomaly_recon_np = anomaly_recon.cpu().numpy().flatten()\n",
    "\n",
    "    # Plot normal sample reconstruction.\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(normal_series_np, label=\"Original Normal\")\n",
    "    plt.plot(normal_recon_np, label=\"Reconstructed Normal\", linestyle=\"--\")\n",
    "    plt.title(\"Normal Sample Reconstruction (TEST file)\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot anomaly sample reconstruction.\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(anomaly_series_np, label=\"Original Anomaly\")\n",
    "    plt.plot(anomaly_recon_np, label=\"Reconstructed Anomaly\", linestyle=\"--\")\n",
    "    plt.title(\"Anomaly Sample Reconstruction (TEST file)\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def export_memory_coe(memory_tensor: torch.Tensor, coe_path: str):\n",
    "    \"\"\"\n",
    "    Export a (M × H) float32 memory tensor to a .coe file where each float\n",
    "    is formatted as an 8-digit hex of its IEEE-754 bits.\n",
    "    \n",
    "    memory_tensor:  (memory_size, hidden_size)\n",
    "    coe_path:       path to write, e.g. 'memory_weights.coe'\n",
    "    \"\"\"\n",
    "    # Flatten and move to CPU numpy\n",
    "    vals = memory_tensor.detach().cpu().numpy().flatten()\n",
    "    # Pack floats into uint32 little-endian and format hex\n",
    "    hex_vals = [\n",
    "        format(struct.unpack('<I', struct.pack('<f', float(v)))[0], '08X')\n",
    "        for v in vals\n",
    "    ]\n",
    "    with open(coe_path, 'w') as f:\n",
    "        f.write('memory_initialization_radix=16;\\n')\n",
    "        f.write('memory_initialization_vector=\\n')\n",
    "        for i, h in enumerate(hex_vals):\n",
    "            sep = ',' if i < len(hex_vals) - 1 else ';'\n",
    "            f.write(f'{h}{sep}\\n')\n",
    "    print(f\"Exported memory weights → {coe_path}\")\n",
    "\n",
    "# … after saving your .pth …\n",
    "torch.save(model.state_dict(), 'tsmae_weights.pth')\n",
    "print(\"Model weights saved to 'tsmae_weights.pth'\")\n",
    "\n",
    "# Export just the MemoryModule weights:\n",
    "export_memory_coe(model.memory_module.memory, 'memory_weights.coe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.io.arff as arff\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Dataset Definition (ECG5000)\n",
    "# ---------------------------\n",
    "class ECG5000(Dataset):\n",
    "    def __init__(self, mode, split='train'):\n",
    "        \"\"\"\n",
    "        mode: 'normal', 'anomaly', or 'all'. \n",
    "              'all' means do not filter any samples (both normal and anomaly).\n",
    "        split: 'train' to load training data; 'test' to load test data.\n",
    "        \"\"\"\n",
    "        assert mode in ['normal', 'anomaly', 'all']\n",
    "        assert split in ['train', 'test']\n",
    "        \n",
    "        # Select the file based on the split.\n",
    "        if split == 'train':\n",
    "            file_path = '/kaggle/input/ecg50000/ECG5000_TRAIN.arff'\n",
    "        else:\n",
    "            file_path = '/kaggle/input/ecg50000/ECG5000_TEST.arff'\n",
    "        \n",
    "        data, meta = arff.loadarff(file_path)\n",
    "        df = pd.DataFrame(data, columns=meta.names())\n",
    "        \n",
    "        # Rename the label column.\n",
    "        new_columns = list(df.columns)\n",
    "        new_columns[-1] = 'target'\n",
    "        df.columns = new_columns\n",
    "        \n",
    "        # Filter samples based on mode.\n",
    "        if mode == 'normal':\n",
    "            df = df[df.target == b'1'].drop(labels='target', axis=1)\n",
    "        elif mode == 'anomaly':\n",
    "            df = df[df.target != b'1'].drop(labels='target', axis=1)\n",
    "        else:  # mode == 'all'\n",
    "            df = df.drop(labels='target', axis=1)\n",
    "        \n",
    "        # Convert DataFrame to a numpy array of type float32.\n",
    "        self.X = df.astype(np.float32).to_numpy()\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Each sample is reshaped as (sequence_length, 1)\n",
    "        sample = torch.from_numpy(self.X[index]).unsqueeze(-1)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def get_torch_tensor(self):\n",
    "        return torch.from_numpy(self.X)\n",
    "\n",
    "# ---------------------------\n",
    "# Memory Module\n",
    "# ---------------------------\n",
    "class MemoryModule(nn.Module):\n",
    "    def __init__(self, memory_size, hidden_size, sparsity_threshold=0.05):\n",
    "        \"\"\"\n",
    "        memory_size: Number of memory items.\n",
    "        hidden_size: Dimensionality of each memory item.\n",
    "        sparsity_threshold: Threshold for rectifying the addressing vector.\n",
    "        \"\"\"\n",
    "        super(MemoryModule, self).__init__()\n",
    "        self.memory_size = memory_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sparsity_threshold = sparsity_threshold\n",
    "        # Initialize learnable memory items.\n",
    "        self.memory = nn.Parameter(torch.randn(memory_size, hidden_size))\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        z: latent representation from encoder, shape (batch, hidden_size)\n",
    "        Returns:\n",
    "          z_hat: recombined latent representation from memory.\n",
    "          q: sparse addressing vector, shape (batch, memory_size)\n",
    "        \"\"\"\n",
    "        # Compute similarity scores between latent vector and memory items.\n",
    "        sim = torch.matmul(z, self.memory.t())\n",
    "        # Softmax to obtain addressing weights.\n",
    "        q = nn.functional.softmax(sim, dim=1)\n",
    "        # Rectify: subtract threshold and zero out negatives.\n",
    "        q = torch.max(q - self.sparsity_threshold, torch.zeros_like(q))\n",
    "        # Normalize so that each row sums to 1.\n",
    "        q = q / (q.sum(dim=1, keepdim=True) + 1e-8)\n",
    "        # Recombine memory items.\n",
    "        z_hat = torch.matmul(q, self.memory)\n",
    "        return z_hat, q\n",
    "\n",
    "# ---------------------------\n",
    "# TSMAE Model\n",
    "# ---------------------------\n",
    "class TSMAE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, memory_size, sparsity_threshold=0.05, sparsity_factor=0.001):\n",
    "        \"\"\"\n",
    "        input_size: Dimension of each time step (e.g., 1)\n",
    "        hidden_size: Dimension of the latent representation.\n",
    "        memory_size: Number of memory items.\n",
    "        sparsity_threshold: Threshold used in the memory module.\n",
    "        sparsity_factor: Weight for the sparsity penalty in the loss.\n",
    "        \"\"\"\n",
    "        super(TSMAE, self).__init__()\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        \n",
    "        # Encoder: a single-layer LSTM.\n",
    "        self.encoder = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        # Memory module.\n",
    "        self.memory_module = MemoryModule(memory_size, hidden_size, sparsity_threshold)\n",
    "        # Decoder: a single-layer LSTM.\n",
    "        self.decoder = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        # Project decoder output back to the input space.\n",
    "        self.output_layer = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Input tensor of shape (batch, seq_len, input_size)\n",
    "        Returns:\n",
    "          x_recon: Reconstructed sequence, shape (batch, seq_len, input_size)\n",
    "          q: Sparse addressing vector from memory, shape (batch, memory_size)\n",
    "          z: Latent representation from encoder, shape (batch, hidden_size)\n",
    "          z_hat: Recombined latent representation from memory, shape (batch, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # Encode: use the final hidden state as the latent vector.\n",
    "        _, (h_n, _) = self.encoder(x)\n",
    "        z = h_n[-1]\n",
    "        # Apply memory module.\n",
    "        z_hat, q = self.memory_module(z)\n",
    "        # For decoding, repeat z_hat over the sequence length.\n",
    "        z_hat_seq = z_hat.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        dec_out, _ = self.decoder(z_hat_seq)\n",
    "        x_recon = self.output_layer(dec_out)\n",
    "        return x_recon, q, z, z_hat\n",
    "\n",
    "    def loss_function(self, x, x_recon, q):\n",
    "        rec_loss = torch.mean((x - x_recon)**2)\n",
    "        sparsity_loss = torch.mean(torch.log(1 + q**2))\n",
    "        loss = rec_loss + self.sparsity_factor * sparsity_loss\n",
    "        return loss, rec_loss, sparsity_loss\n",
    "\n",
    "# ---------------------------\n",
    "# Training Setup\n",
    "# ---------------------------\n",
    "def train_model(model, dataloader, optimizer, device, num_epochs=50):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_recon, q, z, z_hat = model(batch)\n",
    "            loss, rec_loss, sparsity_loss = model.loss_function(batch, x_recon, q)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch.size(0)\n",
    "        avg_loss = epoch_loss / len(dataloader.dataset)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} Loss: {avg_loss:.6f}\")\n",
    "    return train_losses\n",
    "\n",
    "# ---------------------------\n",
    "# Main Execution\n",
    "# ---------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters.\n",
    "    input_size = 1           # Each time step has 1 feature.\n",
    "    hidden_size = 10         # Latent representation dimension.\n",
    "    memory_size = 20         # Number of memory items.\n",
    "    sparsity_threshold = 0.05\n",
    "    sparsity_factor = 0.001\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # ---------------------------\n",
    "    # Training: Use the TRAIN file with both normal and anomaly samples.\n",
    "    # ---------------------------\n",
    "    train_dataset = ECG5000(mode='all', split='train')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    # Initialize the TSMAE model.\n",
    "    model = TSMAE(input_size, hidden_size, memory_size, sparsity_threshold, sparsity_factor)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Check for available device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Training on device:\", device)\n",
    "\n",
    "    # Train the model.\n",
    "    train_losses = train_model(model, train_loader, optimizer, device, num_epochs=num_epochs)\n",
    "\n",
    "    # Plot training loss.\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training Loss vs Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    # ---------------------------\n",
    "    # Save the Model Weights\n",
    "    # ---------------------------\n",
    "    torch.save(model.state_dict(), 'tsmae_weights.pth')\n",
    "    print(\"Model weights saved to 'tsmae_weights.pth'\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Evaluation: Use the TEST file with both normal and anomaly samples.\n",
    "    # ---------------------------\n",
    "    test_dataset = ECG5000(mode='all', split='test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    total_test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            x_recon, q, z, z_hat = model(batch)\n",
    "            loss, rec_loss, sparsity_loss = model.loss_function(batch, x_recon, q)\n",
    "            total_test_loss += loss.item() * batch.size(0)\n",
    "    avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "    print(f\"Average test loss on TEST file: {avg_test_loss:.6f}\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Plotting Reconstruction Comparisons (TEST file)\n",
    "    # ---------------------------\n",
    "    # Evaluate one normal sample from the test file.\n",
    "    normal_dataset_eval = ECG5000(mode='normal', split='test')\n",
    "    normal_sample = normal_dataset_eval[0].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        normal_recon, normal_q, _, _ = model(normal_sample)\n",
    "    normal_series_np = normal_sample.cpu().numpy().flatten()\n",
    "    normal_recon_np = normal_recon.cpu().numpy().flatten()\n",
    "\n",
    "    # Evaluate one anomaly sample from the test file.\n",
    "    anomaly_dataset_eval = ECG5000(mode='anomaly', split='test')\n",
    "    anomaly_sample = anomaly_dataset_eval[0].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        anomaly_recon, anomaly_q, _, _ = model(anomaly_sample)\n",
    "    anomaly_series_np = anomaly_sample.cpu().numpy().flatten()\n",
    "    anomaly_recon_np = anomaly_recon.cpu().numpy().flatten()\n",
    "\n",
    "    # Plot normal sample reconstruction.\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(normal_series_np, label=\"Original Normal\")\n",
    "    plt.plot(normal_recon_np, label=\"Reconstructed Normal\", linestyle=\"--\")\n",
    "    plt.title(\"Normal Sample Reconstruction (TEST file)\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot anomaly sample reconstruction.\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(anomaly_series_np, label=\"Original Anomaly\")\n",
    "    plt.plot(anomaly_recon_np, label=\"Reconstructed Anomaly\", linestyle=\"--\")\n",
    "    plt.title(\"Anomaly Sample Reconstruction (TEST file)\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3235299,
     "sourceId": 5626903,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
